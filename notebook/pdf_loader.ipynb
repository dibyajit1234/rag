{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e152f1",
   "metadata": {},
   "source": [
    "#Rag pipelines :- data ingesion to vector db pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "164b791c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-10 10:42:43.644671: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-10 10:42:43.746714: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/dibyajit/anaconda3/lib/python3.12/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "2025-12-10 10:42:46.758877: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7e11b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 3 pdf files to process \n",
      "\n",
      "processing : Document from sayan_removed (1).pdf\n",
      "\n",
      "loaded 12 pages\n",
      "\n",
      "processing : 2712264064.pdf\n",
      "\n",
      "loaded 3 pages\n",
      "\n",
      "processing : socigo report.pdf\n",
      "\n",
      "loaded 22 pages\n",
      "\n",
      "all document are loaded : 37\n"
     ]
    }
   ],
   "source": [
    "### read all pdf's inside the directory\n",
    "def process_all_pdfs (pdf_directory):\n",
    "    \"\"\"Process all pdf files in a directory\"\"\"\n",
    "    all_documents=[]\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    #find all pdf files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f'found {len(pdf_files)} pdf files to process \\n')\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"processing : {pdf_file.name}\\n\")\n",
    "        try:\n",
    "            loader = PyMuPDFLoader(str(pdf_file))\n",
    "            document = loader.load()\n",
    "            #add source info to metadata\n",
    "            for doc in document:\n",
    "                doc.metadata['source_file']=pdf_file.name\n",
    "                doc.metadata['file_type']='pdf'\n",
    "\n",
    "            all_documents.extend(document)\n",
    "            print(f\"loaded {len(document)} pages\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"error occured {e}\")\n",
    "    print(f\"all document are loaded : {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "all_pdf_documents = process_all_pdfs('../../data/pdf_files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f32dd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='82 \n",
      "Possible questions from Study Materials \n",
      "QB  \n",
      "Sl. No. \n",
      "Question \n",
      "Question \n",
      "Marks \n",
      " \n",
      "Module 3 \n",
      " \n",
      "1 \n",
      "Define Grammar with an example  \n",
      "2 \n",
      "2 \n",
      " Define regular grammar with an example. \n",
      "2 \n",
      "3 \n",
      " Define context-free grammar with an example. \n",
      "2 \n",
      "4 \n",
      " Define Chomsky Normal Form with an example \n",
      "2 \n",
      "5 \n",
      " Define Greibach Normal Form with an example. \n",
      "2 \n",
      "6 \n",
      " Define ambiguous grammar with an example. \n",
      "2 \n",
      "7 \n",
      " Define derivation tree with an example. \n",
      "2 \n",
      "8 \n",
      " Define left most derivation with an example. \n",
      "2 \n",
      "9 \n",
      " Define right most derivation with an example \n",
      "2 \n",
      "10 \n",
      " Define setential form with an example. \n",
      "2 \n",
      "11 \n",
      " Define unit production with an example. \n",
      "2 \n",
      "12 \n",
      " Define Context Sensitive Grammar with an example. \n",
      "2 \n",
      "13 \n",
      " State Church Turing thesis. \n",
      "2 \n",
      "14 \n",
      " Define Universal Turing Machine. \n",
      "2 \n",
      "15 \n",
      " Define Universal Language. \n",
      "2 \n",
      "16 \n",
      " State Ogden’s Lemma. \n",
      "2 \n",
      "17 \n",
      " State Pumping Lemma for Context Free Languages \n",
      "2 \n",
      "18 \n",
      " What do you mean by Halting problem of Turing Machines? \n",
      "2 \n",
      "19 \n",
      " What do you mean by decidable problem? \n",
      "2 \n",
      "20 \n",
      " What do you mean by undecidable problem? \n",
      "2 \n",
      "21 \n",
      "Explain Recursive Languages. \n",
      "2 \n",
      "22 \n",
      " Explain left recursive grammar with an example. \n",
      "2 \n",
      "23 \n",
      "Explain yield of a derivation tree with an example.  \n",
      "2 \n",
      " \n",
      "Module 1 \n",
      " \n",
      "24 \n",
      "What is a Finite Automata? \n",
      "2 \n",
      "25 \n",
      "What do you mean by Alphabet? \n",
      "2 \n",
      "26 \n",
      "What do you mean by a String in Automata Theory? \n",
      "2 \n",
      "27 \n",
      "What is power of an alphabet? \n",
      "2 \n",
      "28 \n",
      "What do you mean by a language? \n",
      "2 \n",
      "29 \n",
      "What is a problem as per central concept of automata theory? \n",
      "2 \n",
      "30 \n",
      "What is Deterministic Finite Automata? \n",
      "2 \n",
      "31 \n",
      "What is Non-deterministic finite automata? \n",
      "2 \n",
      "32 \n",
      "What is Transition function? \n",
      "2 \n",
      "33 \n",
      "What do you mean by acceptance of a string by a finite \n",
      "automata? \n",
      "2 \n",
      "34 \n",
      "What is transition diagram? \n",
      "2 \n",
      "35 \n",
      "What is Transition table? \n",
      "2 \n",
      "36 \n",
      "What do you mean by an extended transition function? \n",
      "2 \n",
      "37 \n",
      "What is regular language? \n",
      "2 \n",
      "38 \n",
      "What is the difference between extended transition function of \n",
      "DFA and NFA? \n",
      "2' metadata={'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../../data/pdf_files/Document from sayan_removed (1).pdf', 'file_path': '../../data/pdf_files/Document from sayan_removed (1).pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-27T15:19:09+00:00', 'trapped': '', 'modDate': 'D:20251027151909Z', 'creationDate': '', 'page': 0, 'source_file': 'Document from sayan_removed (1).pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(all_pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad5403",
   "metadata": {},
   "outputs": [],
   "source": [
    "### text splitting get into chinks\n",
    "\n",
    "def split_documents(documents, chunk_size=1000,chunk_overlap = 200):\n",
    "    \"\"\"split documents into smaller chunks for better rag performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators = [\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    if split_docs:\n",
    "        print(f\"\\n example chunks\")\n",
    "        print(f\"Content :{split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata:{split_docs[0].metadata}\")\n",
    "    else:\n",
    "        print(\"error occured in chunking\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f09c19f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 37 documents into 72 chunks\n",
      "\n",
      " example chunks\n",
      "Content :82 \n",
      "Possible questions from Study Materials \n",
      "QB  \n",
      "Sl. No. \n",
      "Question \n",
      "Question \n",
      "Marks \n",
      " \n",
      "Module 3 \n",
      " \n",
      "1 \n",
      "Define Grammar with an example  \n",
      "2 \n",
      "2 \n",
      " Define regular grammar with an example. \n",
      "2 \n",
      "3 \n",
      " Define co...\n",
      "Metadata:{'producer': 'iLovePDF', 'creator': '', 'creationdate': '', 'source': '../../data/pdf_files/Document from sayan_removed (1).pdf', 'file_path': '../../data/pdf_files/Document from sayan_removed (1).pdf', 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2025-10-27T15:19:09+00:00', 'trapped': '', 'modDate': 'D:20251027151909Z', 'creationDate': '', 'page': 0, 'source_file': 'Document from sayan_removed (1).pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180085d1",
   "metadata": {},
   "source": [
    "Embedding and vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c4fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List,Dict,Any,Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f9ec45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model sentence-transformers/all-MiniLM-L6-v2\n",
      "\n",
      "Model loaded successfully , Embedding dimensions:384\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x7f7b6e9d9ac0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handels document embedding generation using SentenceTransformers\"\"\"\n",
    "    \n",
    "    def __init__(self,model_name:str =\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model name: Hugging face model name for system embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the sentence transformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model {self.model_name}\\n\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully , Embedding dimensions:{self.model.get_sentence_embedding_dimension()}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model{self.model_name} {e}\\n\")\n",
    "            raise \n",
    "\n",
    "    def generate_embeddings(self,texts:List[str])-> np.ndarray:\n",
    "        \"\"\"Generate embeddings for  a list of strings\"\"\"\n",
    "\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not  loaded\")\n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    # def get_sentence_embedding_dimension(self)-> int:\n",
    "    #     \"\"\"Get the embedding dimensions of the model\"\"\"\n",
    "\n",
    "    #     if not self.model:\n",
    "    #         raise ValueError(\"Model not loaded\")\n",
    "    #     return self.model.get_sentence_embedding_dimension()\n",
    "\n",
    "    #initialize the embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3340ed9",
   "metadata": {},
   "source": [
    "Vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1872e8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized, Collection:pdf_documents\n",
      "Exiting documents in collection: 400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x7f7b6dd52a20>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a chromaDB vector store\"\"\"\n",
    "    def __init__(self,collection_name:str = \"pdf_documents\",persist_directory : str = \"../../data/vector_store\"):\n",
    "        \"\"\"Initialize the vector store\"\"\"\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self.initialize_store()\n",
    "    def initialize_store(self):\n",
    "        \"\"\"Initialize chromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            #create persistent chroma client\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            #get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name= self.collection_name,\n",
    "                metadata={\"description\":\"pdf document embeddings for rag\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized, Collection:{self.collection_name}\")\n",
    "            print(f\"Exiting documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store :{e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self,documents:List[Any],embeddings:np.ndarray):\n",
    "        \"\"\" Add documents and other embeddings to the vedtor db\n",
    "            Args:\n",
    "            documents: List of langchain documents\n",
    "            embeddings: Corresponding embeddings to the documents\n",
    "        \"\"\"\n",
    "        if len(documents)!=len(embeddings):\n",
    "            raise ValueError(\"Number of documents must be equal to the number of embeddings\")\n",
    "        print(f\"adding {len(documents)} documents to the vector store\")\n",
    "\n",
    "        #prepare data for chromaDB\n",
    "        ids=[]\n",
    "        metadatas =[]\n",
    "        documents_text =[]\n",
    "        embeddings_list=[]\n",
    "\n",
    "        for i,(doc,embedding) in enumerate(zip(documents,embeddings)):\n",
    "            #generate unique id\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index']=i\n",
    "            metadata['content_length']=len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            #document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            #Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        #add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total document in collection {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store :{e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore();\n",
    "vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be90dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 72 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bab66048cd54a4293faa286340e1fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape (72, 384)\n",
      "adding 72 documents to the vector store\n",
      "successfully added 72 documents to vector store\n",
      "Total document in collection 472\n"
     ]
    }
   ],
   "source": [
    "#convert the chunks to embeddings\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "#Generate the embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "#store in the vector database\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c808e12",
   "metadata": {},
   "source": [
    "Retrever pipeline for VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "587d67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriver:\n",
    "    def __init__(self,vector_store:VectorStore,embedding_manager:EmbeddingManager):\n",
    "        \"\"\" Initialize the retriver\n",
    "        args:\n",
    "            vector store: vector store contains document embeddings\n",
    "            embedding manager: manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self,query:str,top_k:int=5,score_threshold:float=0.0)->List[Dict[str,Any]]:\n",
    "        \"\"\" Retrive relavent documents for a query\n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return \n",
    "            score_threshold: Minimum similarity score threshold\n",
    "        Returns:\n",
    "            List of dictionaries containing retrived documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retriving documents for query{query}\")\n",
    "        print(f\"Top k: {top_k},Score threshold:{score_threshold}\")\n",
    "\n",
    "        #generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        #search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings = [query_embedding.tolist()],\n",
    "                n_results = top_k\n",
    "            )\n",
    "            #process results\n",
    "            retrived_docs =[]\n",
    "\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i , (document, metadata, distance, id) in enumerate(zip(documents,metadatas,distances,ids)):\n",
    "                    #convert distance to similarity score\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score>=score_threshold:\n",
    "                        retrived_docs.append({\n",
    "                            'id':id,\n",
    "                            'content':document,\n",
    "                            'metadata':metadata,\n",
    "                            'similarity_score':similarity_score,\n",
    "                            'distance':distance,\n",
    "                            'rank':i+1\n",
    "                        })\n",
    "                print(f\"Retrived {len(retrived_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            return retrived_docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrival {e}\")\n",
    "            return []\n",
    "        \n",
    "rag_retriver = RAGRetriver(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a5cda9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriving documents for queryThis is to certify that the project report entitled “SociGo-Social Media Platform” submitted by\n",
      "Top k: 5,Score threshold:0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21576a8325a747beab1add74468a0d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape (1, 384)\n",
      "Retrived 5 documents (after filtering)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_cae6eac7_59',\n",
       "  'content': 'III \\n \\nApproval Certificate \\n \\n \\nThis is to certify that the project report entitled “SociGo-Social Media Platform” submitted by \\nArpan Pramanik(Roll:12023002001104), Dibyajit Nandi(Roll:12023002001072) and Sanglap \\nGhosh (Roll:12023002001057)  in partial fulfillment of the requirements of the degree of \\nBachelor of Technology in Computer Science & Engineering from University of Engineering \\nand Management, Jaipur was conducted in a systematic and procedural manner to the best of our',\n",
       "  'metadata': {'format': 'PDF 1.7',\n",
       "   'content_length': 488,\n",
       "   'creationdate': '2025-11-06T22:54:21+05:30',\n",
       "   'producer': '3.0.30 (5.1.14)',\n",
       "   'author': 'WPS Office',\n",
       "   'keywords': '',\n",
       "   'title': '',\n",
       "   'creationDate': \"D:20251106225421+05'30'\",\n",
       "   'source_file': 'socigo report.pdf',\n",
       "   'page': 2,\n",
       "   'trapped': '',\n",
       "   'doc_index': 59,\n",
       "   'source': '../../data/pdf_files/socigo report.pdf',\n",
       "   'total_pages': 22,\n",
       "   'creator': 'Microsoft® Word 2021',\n",
       "   'file_path': '../../data/pdf_files/socigo report.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'subject': '',\n",
       "   'moddate': '2025-11-06T21:13:20+01:00',\n",
       "   'modDate': \"D:20251106211320+01'00'\"},\n",
       "  'similarity_score': 0.5213544070720673,\n",
       "  'distance': 0.47864559292793274,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_d4865a02_120',\n",
       "  'content': 'easily fit into evolving user demands and changes in technology. Its modular architecture will \\nallow for easy integration of future features that include AI-driven content recommendations, \\nlive streaming, advanced analytics, and monetization tools. \\n6.2 Limitations and Future Scope  \\nAlthough SociGo effectively implements the core features of any modern social media platform, \\nthere are still some limitations to the current version. The backend of the application is functional',\n",
       "  'metadata': {'doc_index': 120,\n",
       "   'file_path': '../../data/pdf_files/socigo report.pdf',\n",
       "   'creator': 'Microsoft® Word 2021',\n",
       "   'producer': '3.0.30 (5.1.14)',\n",
       "   'source_file': 'socigo report.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'creationDate': \"D:20251106225421+05'30'\",\n",
       "   'author': 'WPS Office',\n",
       "   'format': 'PDF 1.7',\n",
       "   'total_pages': 22,\n",
       "   'title': '',\n",
       "   'trapped': '',\n",
       "   'modDate': \"D:20251106211320+01'00'\",\n",
       "   'moddate': '2025-11-06T21:13:20+01:00',\n",
       "   'subject': '',\n",
       "   'creationdate': '2025-11-06T22:54:21+05:30',\n",
       "   'keywords': '',\n",
       "   'source': '../../data/pdf_files/socigo report.pdf',\n",
       "   'content_length': 483,\n",
       "   'page': 19},\n",
       "  'similarity_score': 0.3425254821777344,\n",
       "  'distance': 0.6574745178222656,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_70d8d887_32',\n",
       "  'content': 'I \\n \\nSOCIGO- SOCIAL MEDIA PLATFORM \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nJAIPUR\\nUNIVERSITY OF ENGINEERING & MANAGEMENT,',\n",
       "  'metadata': {'trapped': '',\n",
       "   'title': '',\n",
       "   'author': 'WPS Office',\n",
       "   'keywords': '',\n",
       "   'doc_index': 32,\n",
       "   'file_path': '../../data/pdf_files/socigo report.pdf',\n",
       "   'content_length': 144,\n",
       "   'moddate': '2025-11-06T21:13:20+01:00',\n",
       "   'total_pages': 22,\n",
       "   'format': 'PDF 1.7',\n",
       "   'source_file': 'socigo report.pdf',\n",
       "   'creationdate': '2025-11-06T22:54:21+05:30',\n",
       "   'modDate': \"D:20251106211320+01'00'\",\n",
       "   'creator': 'Microsoft® Word 2021',\n",
       "   'source': '../../data/pdf_files/socigo report.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'producer': '3.0.30 (5.1.14)',\n",
       "   'page': 0,\n",
       "   'subject': '',\n",
       "   'creationDate': \"D:20251106225421+05'30'\"},\n",
       "  'similarity_score': 0.2629354000091553,\n",
       "  'distance': 0.7370645999908447,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_00e5aed6_32',\n",
       "  'content': 'I \\n \\nSOCIGO- SOCIAL MEDIA PLATFORM \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nJAIPUR\\nUNIVERSITY OF ENGINEERING & MANAGEMENT,',\n",
       "  'metadata': {'modDate': \"D:20251106211320+01'00'\",\n",
       "   'source_file': 'socigo report.pdf',\n",
       "   'keywords': '',\n",
       "   'creator': 'Microsoft® Word 2021',\n",
       "   'moddate': '2025-11-06T21:13:20+01:00',\n",
       "   'file_path': '../../data/pdf_files/socigo report.pdf',\n",
       "   'page': 0,\n",
       "   'author': 'WPS Office',\n",
       "   'creationdate': '2025-11-06T22:54:21+05:30',\n",
       "   'file_type': 'pdf',\n",
       "   'trapped': '',\n",
       "   'total_pages': 22,\n",
       "   'format': 'PDF 1.7',\n",
       "   'doc_index': 32,\n",
       "   'producer': '3.0.30 (5.1.14)',\n",
       "   'creationDate': \"D:20251106225421+05'30'\",\n",
       "   'source': '../../data/pdf_files/socigo report.pdf',\n",
       "   'title': '',\n",
       "   'content_length': 144,\n",
       "   'subject': ''},\n",
       "  'similarity_score': 0.2629354000091553,\n",
       "  'distance': 0.7370645999908447,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_2fcdcaba_32',\n",
       "  'content': 'I \\n \\nSOCIGO- SOCIAL MEDIA PLATFORM \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nJAIPUR\\nUNIVERSITY OF ENGINEERING & MANAGEMENT,',\n",
       "  'metadata': {'content_length': 144,\n",
       "   'creationdate': '2025-11-06T22:54:21+05:30',\n",
       "   'file_path': '../../data/pdf_files/socigo report.pdf',\n",
       "   'producer': '3.0.30 (5.1.14)',\n",
       "   'page': 0,\n",
       "   'subject': '',\n",
       "   'creationDate': \"D:20251106225421+05'30'\",\n",
       "   'keywords': '',\n",
       "   'modDate': \"D:20251106211320+01'00'\",\n",
       "   'file_type': 'pdf',\n",
       "   'trapped': '',\n",
       "   'format': 'PDF 1.7',\n",
       "   'total_pages': 22,\n",
       "   'creator': 'Microsoft® Word 2021',\n",
       "   'moddate': '2025-11-06T21:13:20+01:00',\n",
       "   'title': '',\n",
       "   'source_file': 'socigo report.pdf',\n",
       "   'author': 'WPS Office',\n",
       "   'doc_index': 32,\n",
       "   'source': '../../data/pdf_files/socigo report.pdf'},\n",
       "  'similarity_score': 0.2629354000091553,\n",
       "  'distance': 0.7370645999908447,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriver.retrieve(\"This is to certify that the project report entitled “SociGo-Social Media Platform” submitted by\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d591844",
   "metadata": {},
   "source": [
    "Integration vector db context pipeline output with llm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "25d348cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple rag pipeline with groq llm\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "#Initialize the groq LLM\n",
    "groq_api_key = os.getenv(\"GROQ_API\")\n",
    "llm = ChatGroq(api_key=groq_api_key,model =\"openai/gpt-oss-120b\",temperature=0.1,max_tokens=1024)\n",
    "\n",
    "#Simple rag function to retrive context to generate response\n",
    "def rag_simple(query, retriever, llm,top_k=3):\n",
    "    ##retrive the context\n",
    "    results = retriever.retrieve(query,top_k=top_k)\n",
    "    context = \"\\n\\n\".join(doc['content'] for doc in results) if results else \"\"\n",
    "    if not context:\n",
    "        print(\"No relavent context found\\n\")\n",
    "    #Generate the answer using groq llm\n",
    "    prompt =f\"\"\"Use the following context to answer the questions concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke([prompt.format(context =context, query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "720b0f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriving documents for queryThis project presents SociGo-a cross-platform,\n",
      "Top k: 3,Score threshold:0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5bdef9abd24712b2cc86f3b2189991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape (1, 384)\n",
      "Retrived 2 documents (after filtering)\n",
      "This project presents SociGo—a cross‑platform social media platform built with a modular architecture to meet evolving user demands and support future AI‑driven features.\n"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"This project presents SociGo-a cross-platform,\",rag_retriver,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2e521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
